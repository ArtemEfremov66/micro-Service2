# 1. Обеспечить разработку
1. Система контроля версий:

GitHub: Используется для хранения исходного кода. Каждый сервис имеет свой отдельный репозиторий.  

2. Система непрерывной интеграции и непрерывной поставки (CI/CD):

GitHub Actions: Используется для автоматизации процессов сборки, тестирования и развертывания.

3. Управление секретами:

HashiCorp Vault: Используется для безопасного хранения и управления секретными данными.

Обоснование: Vault - проверенное решение для управления секретами, предоставляет надежное хранилище и механизмы аудита доступа. Интегрируется с GitHub Actions через плагины или API.
Взаимодействие компонентов:

Код: Разработчики работают с локальными копиями репозиториев GitHub и отправляют изменения (commits, pull requests) в удаленный репозиторий.
Триггеры: GitHub Actions конфигурируется для автоматического запуска сборки при определенных событиях в репозитории (например, push в branch, создание pull request).
Сборка: GitHub Actions запускает workflow, определенный в YAML файле (.github/workflows/). Workflow может включать следующие шаги:
Получение исходного кода.
Получение секретов из HashiCorp Vault (если необходимо).
Сборка проекта с использованием указанного Docker образа.
Запуск тестов.
Развертывание приложения (например, в AWS, Azure, OpenAI Cloud).
Артефакты: После успешной сборки, GitHub Actions может сохранить артефакты (например, Docker image, JAR файл) для дальнейшего использования.

#### Соответствие требованиям:

Облачная система: GitHub и GitHub Actions - облачные сервисы. HashiCorp Vault может быть развернут в облаке или локально.  
Система контроля версий Git: GitHub основан на Git.  
Репозиторий на каждый сервис: Рекомендуется структура “один репозиторий - один микросервис”.  
Запуск сборки по событию из системы контроля версий: GitHub Actions поддерживает триггеры по различным событиям (push, pull request).  
Запуск сборки по кнопке с указанием параметров: workflow_dispatch trigger позволяет запускать workflow вручную через интерфейс GitHub Actions с передачей параметров.  
Возможность привязать настройки к каждой сборке: Параметры сборки можно передавать через переменные окружения или аргументы командной строки.  
Возможность создания шаблонов для различных конфигураций сборок: Можно создавать workflow templates, которые можно повторно использовать в разных репозиториях.   Также можно использовать reusable workflows.  
Возможность безопасного хранения секретных данных (пароли, ключи доступа): HashiCorp Vault предоставляет безопасное хранилище для секретов, которые могут быть использованы в процессе сборки. Альтернативно, можно использовать встроенные секреты GitHub Actions.  
Несколько конфигураций для сборки из одного репозитория: Можно использовать разные workflow YAML файлы или условную логику внутри одного workflow для различных конфигураций сборки (например, сборка для разработки, staging, production).  
Кастомные шаги при сборке: GitHub Actions поддерживает произвольные shell команды и действия (actions), позволяющие выполнять любые необходимые шаги при сборке.
Собственные докер-образы для сборки проектов: Можно указать собственный Docker образ для сборки в GitHub Actions workflow.  
Возможность развернуть агентов сборки на собственных серверах: GitHub Actions поддерживает self-hosted runners, которые можно развернуть на собственных серверах для выполнения сборок.  
Возможность параллельного запуска нескольких сборок: GitHub Actions позволяет параллельно запускать несколько workflow.  
Возможность параллельного запуска тестов: Можно разбить тесты на несколько этапов и запустить их параллельно в GitHub Actions workflow.  

# 2. Логи

Для централизованного сбора, хранения и анализа логов удобно использовать стек EFK (Elasticsearch, Fluentd, Kibana).

Компоненты и взаимодействие:

1. Elasticsearch (центральное хранилище):

Хранит все логи в индексированном формате.
Обеспечивает быстрый поиск и анализ данных.
Масштабируется горизонтально для обработки больших объемов логов.

2. Fluentd (сбор логов):

Устанавливается на каждом хосте, где работают микросервисы.
Собирает логи из stdout приложений.
Буферизует логи и отправляет их в Elasticsearch.
Поддерживает множество входных и выходных плагинов для гибкой настройки.

3. Kibana (пользовательский интерфейс):

Предоставляет веб-интерфейс для визуализации и анализа логов, хранящихся в Elasticsearch.
Позволяет создавать дашборды, фильтровать логи и выполнять сложные поисковые запросы.
Поддерживает ролевую модель для предоставления доступа разработчикам.
Позволяет сохранять поисковые запросы и делиться ссылками на них.

#### Реализация требований:

Сбор логов в центральное хранилище со всех хостов: Fluentd, установленный на каждом хосте, пересылает логи в Elasticsearch.  
Минимальные требования к приложениям, сбор логов из stdout: Fluentd собирает логи непосредственно из stdout приложений, не требуя никаких изменений в коде. Приложения должны просто выводить логи в stdout.  
Гарантированная доставка логов до центрального хранилища: Fluentd имеет механизмы буферизации и повторной отправки логов в случае сбоев Elasticsearch.   Поддерживает использование файловой системы или Redis для буферизации. Это обеспечивает надежную доставку логов даже при временной недоступности центрального хранилища.  
Обеспечение поиска и фильтрации по записям логов: Elasticsearch предоставляет мощные возможности поиска и фильтрации по текстовым полям, временным меткам и другим атрибутам логов.  
Обеспечение пользовательского интерфейса с возможностью предоставления доступа разработчикам для поиска по записям логов: Kibana предоставляет веб-интерфейс для доступа к данным, хранящимся в Elasticsearch. Можно настроить ролевую модель для предоставления доступа разработчикам к определенным индексам и дашбордам.  
Возможность дать ссылку на сохранённый поиск по записям логов: Kibana позволяет сохранять поисковые запросы и делиться ссылками на них, что упрощает совместную работу и отладку проблем.  

# 3. Мониторинг

Для комплексного мониторинга состояния хостов и сервисов удобно использовать стек Prometheus + Grafana.

Компоненты и взаимодействие:

1. Prometheus (система сбора метрик и хранения):

Собирает метрики с целевых объектов (хостов и сервисов).
Хранит метрики в виде временных рядов (time-series data).
Предоставляет мощный язык запросов PromQL для анализа метрик.
Alertmanager (часть экосистемы Prometheus) отвечает за обработку и маршрутизацию оповещений.

2. Node Exporter (для сбора метрик хостов):

Устанавливается на каждом хосте, где работают микросервисы.
Экспортирует метрики состояния ресурсов хоста (CPU, RAM, HDD, Network) в формате, понятном Prometheus.

3. Application Exporters (для сбора метрик сервисов):

Предназначены для сбора метрик, специфичных для каждого сервиса.
Могут быть реализованы как HTTP endpoints, возвращающие метрики в формате Prometheus.
Примеры: JMX Exporter (для Java), Prometheus Client Libraries (для разных языков программирования).

4. Grafana (пользовательский интерфейс):

Подключается к Prometheus в качестве источника данных.
Предоставляет веб-интерфейс для визуализации метрик.
Позволяет создавать дашборды с графиками, таблицами и другими визуальными элементами.
Поддерживает настройку оповещений на основе пороговых значений метрик.
Обеспечивает ролевую модель для управления доступом пользователей.

Детали реализации:

Node Exporter: Устанавливается и запускается на каждом хосте. Он автоматически собирает системные метрики и предоставляет их по HTTP адресу (обычно http://<host_ip>:9100/metrics).

Application Exporters: Для каждого сервиса необходимо реализовать механизм экспорта метрик. Это можно сделать, используя клиентские библиотеки Prometheus (например, для Go, Java, Python) или создав простой HTTP endpoint, возвращающий метрики в формате Prometheus. Метрики могут включать в себя:

Количество обработанных запросов.
Время обработки запросов.
Количество ошибок.
Состояние кэша.
Активные соединения с базой данных.

Prometheus Configuration: В prometheus.yml необходимо настроить Prometheus для обнаружения и сбора метрик с Node Exporters и Application Exporters.

#### Соответствие требованиям:

Сбор метрик со всех хостов: Node Exporter устанавливается на каждом хосте и Prometheus собирает с них метрики.  
Сбор метрик состояния ресурсов хостов: Node Exporter предоставляет метрики CPU, RAM, HDD, Network.  
Сбор метрик потребляемых ресурсов для каждого сервиса: Сбор метрик осуществляется через Application Exporters, специфичные для каждого сервиса.  
Сбор метрик, специфичных для каждого сервиса: Application Exporters позволяют собирать любые метрики, специфичные для конкретного сервиса.  
Пользовательский интерфейс с возможностью делать запросы и агрегировать информацию: Grafana предоставляет UI для запросов (PromQL) и агрегации информации.  
Пользовательский интерфейс с возможностью настраивать различные панели для отслеживания состояния системы: Grafana позволяет создавать настраиваемые дашборды с различными панелями (графики, таблицы, gauges и т.д.).
